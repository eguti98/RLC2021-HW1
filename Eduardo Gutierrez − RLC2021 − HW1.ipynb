{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all the necessary libraries for the code\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "from time import sleep\n",
    "from IPython.display import clear_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : :\u001b[43m \u001b[0m|\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "\n",
      "Number of possible States 500\n",
      "Number of possible Actions 6\n"
     ]
    }
   ],
   "source": [
    "# Set and check the enviroment characteristics\n",
    "\n",
    "env = gym.make('Taxi-v3')\n",
    "env.render()\n",
    "print (\"Number of possible States\", env.nS)\n",
    "print (\"Number of possible Actions\", env.nA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "EVALUATE A POLICY GIVEN AN ENVIROMENT AND ITS CHARACTERISTICS\n",
    "\n",
    "Inputs\n",
    "    - policy: Matrix [S , A]\n",
    "    - env: Taxi-v3 enviroment\n",
    "        - env.P : Transition probabilities\n",
    "        - env.P [s] [a] : List of transition tupples (prob, next_state, reward, done)\n",
    "        - env.nS : Number of states\n",
    "        - env.nA : Number of actions\n",
    "    - gamma : Discount factor\n",
    "    - theta : Reference that stops evaluation if the change in the value function change is smaller than it\n",
    "Outputs\n",
    "    - V : Vector value function with lenght of number of steps\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "gamma = 1.0\n",
    "theta = 0.00001\n",
    "\n",
    "def policy_evaluation(policy , env , gamma , theta):\n",
    "\n",
    "    V = np.zeros(env.env.nS)  # Value funcion, where all the elements are 0, with lenght of number of steps\n",
    "    while True:\n",
    "        delta = 0  # Change in Value function\n",
    "        for state in range(env.env.nS):  # For every state in all the states\n",
    "            value = 0  # Set initial value equal to 0\n",
    "            for action,act_prob in enumerate(policy[state]): # For all actions/action probabilities\n",
    "                for prob,next_state,reward,done in env.env.P[state][action]:  # Transition probabilities,state,rewards of each action\n",
    "                    value += act_prob * prob * (reward + gamma * V[next_state])  # Equation\n",
    "            delta = max(delta, np.abs(value-V[state]))\n",
    "            V[state] = value\n",
    "        if delta < theta:  # Reference to stop evaluation process\n",
    "            break\n",
    "\n",
    "    return np.array(V)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "PREDICT AND CALCULATE VALUE OF EVERY ACTION IN A STATE\n",
    "\n",
    "Inputs\n",
    "    - state: State to consider\n",
    "    - V: Vector value function with lenght of number of steps\n",
    "Outputs\n",
    "    - A : Vector value of each action with length of number of actions\n",
    "    \n",
    "\"\"\"\n",
    "\n",
    "def prediction(state, V):\n",
    "\n",
    "    A = np.zeros(env.env.nA)  # Vector value of each action, where all the elements are 0, with lenght of number of actions\n",
    "    for a in range(env.env.nA):  # For every action in all the actions\n",
    "        for prob, next_state, reward, done in env.env.P[state][a]:  # Transition probabilities,state,rewards of each action\n",
    "            A[a] += prob * (reward + gamma * V[next_state])  # Equation\n",
    "            \n",
    "    return A\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "POLICY IMPROVEMENT. EVALUATES AND IMPROVES POLICY UNTIL OPTIMAL\n",
    "\n",
    "Inputs\n",
    "    - env : Taxi-v3 enviromen\n",
    "    - policy_evaluation : Policy evaluation function (policy,env,gamma,theta)\n",
    "    - gamma : Discount factor\n",
    "Outputs\n",
    "    (policy, V)\n",
    "        - policy : Optimal policy, matrix of shape [S , A]. Each state (S) contains a valid probability distribution over actions (A)\n",
    "        - V : Value function for the optimal policy.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def policy_improvement(env, policy_evaluation, gamma):\n",
    "    \n",
    "    policy = np.ones([env.env.nS, env.env.nA]) / env.env.nA  # Start with a random policy\n",
    "    while True:\n",
    "        curr_pol_val = policy_evaluation(policy, env, gamma,theta)  # Evaluate the current policy\n",
    "        policy_stable = True  # Check if policy improved (Initially True)\n",
    "        for state in range(env.env.nS):  # For every state in all the states\n",
    "            chosen_act = np.argmax(policy[state])  # Best action (Highest prob) under current policy\n",
    "            act_values = prediction(state,curr_pol_val)  # Use prediction to see future action values\n",
    "            best_act = np.argmax(act_values) # Find best action\n",
    "            if chosen_act != best_act:\n",
    "                policy_stable = False  # Look again\n",
    "            policy[state] = np.eye(env.env.nA)[best_act]  # Update\n",
    "        if policy_stable:\n",
    "            return policy, curr_pol_val\n",
    "    \n",
    "    return policy, np.zeros(env.env.nS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma=0.99\n",
    "pol_iter_policy = policy_improvement(env,policy_evaluation,gamma)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def view(policy):\n",
    "    \n",
    "    penalties, reward = 0, 0\n",
    "    frames = []\n",
    "    done = False\n",
    "    curr_state = env.reset()\n",
    "    while not done:\n",
    "        action = np.argmax(policy[0][curr_state])\n",
    "        state, reward, done, info = env.step(action)\n",
    "        curr_state = state\n",
    "        if reward == -10:\n",
    "            penalties += 1\n",
    "        frames.append({'frame': env.render(mode='ansi'),'state': state,'action': action,'reward': reward})\n",
    "    def animation(frames):\n",
    "        for i, frame in enumerate(frames):\n",
    "            clear_output(wait=True)\n",
    "            print(frame['frame'])\n",
    "            print(f\"Timestep: {i + 1}\")\n",
    "            print(f\"State: {frame['state']}\")\n",
    "            print(f\"Action: {frame['action']}\")\n",
    "            print(f\"Reward: {frame['reward']}\")\n",
    "            sleep(.5)\n",
    "    animation(frames)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35m\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (Dropoff)\n",
      "\n",
      "Timestep: 9\n",
      "State: 410\n",
      "Action: 5\n",
      "Reward: 20\n"
     ]
    }
   ],
   "source": [
    "view(pol_iter_policy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
